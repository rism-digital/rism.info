---
layout: post
title: Digital Musicology bei der Oxford Summer School
date: 2019-09-02
lang: de
post: true
category: events
image: "/images/news-old-website/csm_Oxford_Schola_Musicae_6ee2b1c702.jpg"
old_url: http://www.rism.info/de/startseite/newsdetails/browse/62/article/64/digital-musicology-at-oxford-summer-school.html
email: ''
author: Jennifer Ward (RISM Zentralredaktion)
---

Ich wollte schon seit einiger Zeit an der [Digital Humanities at Oxford Summer School (DHOxSS)](https://www.dhoxss.net/){:target="_blank"} teilnehmen, dank der begeisterten Rezensionen von Bibliotheksfreunden. Ich war froh, als meine Sommerpläne endlich auf die diesjährige Summer School abgestimmt wurden, und ich direkt nach dem [IAML-Kongress in Krakau](/events/2019/08/26/congress-diary-from-iaml-kraków-2019.html){:target="_blank"} nach Oxford fahren konnte. Ich hatte das Glück, ein Stipendium für den [Digital Musicology Strang](https://www.dhoxss.net/digital-musicology){:target="_blank"} zu erhalten, der von Kevin Page einberufen wurde. Ein Bericht, der meine Woche zusammenfasst, wurde auf der [DHOxSS-Webseite](https://www.dhoxss.net/jennifer-ward){:target="_blank"} veröffentlicht. An dieser Stelle möchte ich näher auf die von uns verwendeten Werkzeuge und Techniken eingehen.

Die Woche war grob in drei Module unterteilt, die sich auf digitales Audio, digitale Notation und digitale Beschreibungen konzentrierten. Das Format der Unterrichtseinheiten wurde so gemischt, dass sich die Vorlesungen mit viel Zeit abwechselten, um mit den neuen Werkzeugen zu üben. Eine Vielzahl von Wissenschaftlern und Praktikern teilten mit, wie digitale Werkzeuge in ihrer Forschung verwendet werden, und dann konnten wir die Werkzeuge selbst ausprobieren.

Bei der digitalen Audioverarbeitung werden Audiodateien auf fast mathematische Weise analysiert. Das Endergebnis sind Zahlen, die in einer Datenbank bearbeitet werden können. Dabei haben wir uns tief in den Bereich der Musikinformationsbeschaffung (siehe [ISMIR](http://ismir.net/){:target="_blank"} und [MIREX](https://www.music-ir.org/mirex/wiki/MIREX_HOME){:target="_blank"}), n-Gramme und die Physik der Audiowellen eingearbeitet. Wir haben [Sonic Visualiser](https://www.sonicvisualiser.org/){:target="_blank"} und [Vamp-Plugins](https://vamp-plugins.org/){:target="_blank"} verwendet, um Features aus Tonaufnahmen zu extrahieren. Anschließend können Sie den Computer auffordern, nach Mustern zu suchen, z. B. nach "In nomine"-Melodien in einem Korpus aus Lautenmusik der Renaissance. [Songle](https://songle.jp/){:target="_blank"} und [Chordify](https://chordify.net/){:target="_blank"} sind andere Projekte, die Audiomuster erkennen. Mit letzterem können Sie YouTube-Videos durch Anzeigen der Akkorde begleiten. Das [Baudelaire Song Project](https://www.baudelairesong.org/){:target="_blank"} verfügt über [Visualisierungstools](https://visualisebaudelairesong.bham.ac.uk/){:target="_blank"}, mit denen Sie 1.700 Einträge der Texte des Dichters nach Jahrzehnt, Thema, Genre usw. durchsuchen können.

Computer können auch die Notation analysieren, wenn ihnen zuerst das Lesen beigebracht wurde. Frauke Jürgensen verwendet [kern](https://www.humdrum.org/rep/kern/){:target="_blank"} und [Humdrum](https://www.humdrum.org/){:target="_blank"} zum Kodieren und Verarbeiten der Mensuralnotation. Anschließend können Sie den Computer auffordern, eine Vielzahl von Intervallen, Kadenzen und anderen Aspekten zu zählen, wofür ein Mensch unverhältnismäßig viel Zeit benötigen würde. Tools, die die Standards der [Music Encoding Initiative (MEI)](https://music-encoding.org/){:target="_blank"}, den [Atom-Texteditor](https://atom.io/){:target="_blank"}, den [Verovio-Viewer](https://www.verovio.org/){:target="_blank"} und die [Music21-Bibliothek](https://web.mit.edu/music21/){:target="_blank"} (mit [Python/Jupyter](https://jupyter.org/){:target="_blank"}) verwenden, sind eine weitere Möglichkeit, Musikdetails so zu kodieren, dass eine Analyse durchgeführt werden kann.

Das letzte Thema des Workshops konzentrierte sich auf die Welt der Wissensvernetzung: Nicht nur das Hineinwerfen von Informationen ins Netz, sondern auch das Entdecken und Analysieren durch die Verbindung mit anderen Projekten. Dies bringt uns in die Welt des Semantic Web, der verknüpften [Open Data Cloud](https://lod-cloud.net/#){:target="_blank"} ([RISM ist dabei](https://lod-cloud.net/dataset/rism){:target="_blank"}), des Resource Description Framework (RDF), SPARQL-Abfragen und anderer Dinge, die Ihnen vielleicht bekannt vorkommen, wenn Sie mit [Big Data-Konzeepten](https://doi.org/10.1093/em/cav071){:target="_blank"} vertraut sind. [Wikidata](https://query.wikidata.org){:target="_blank"} hat eine leicht verständliche Oberfläche, die die Leistungsfähigkeit verknüpfter Daten (klicken Sie auf Beispiele) mit ihrer Karte der [Komponisten nach Geburtsort](https://query.wikidata.org/#%23Music%20composers%20by%20birth%20place%0A%23defaultView%3AMap%0ASELECT%20%3Fitem%20%3FitemLabel%20%3F_coordinates%20%3F_image%20WHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP106%20wd%3AQ36834%3B%20%20%20%23%20occupation%3A%20composer%0A%20%20%20%20%20%20%20%20wdt%3AP18%20%3F_image%3B%20%20%20%23%20with%20an%20image%20depicting%20them%0A%20%20%20%20%20%20%20%20wdt%3AP19%2Fwdt%3AP625%20%3F_coordinates%20%20%20%23%20their%20birthplace%2C%20specifically%20the%20coordinates%20of%20their%20birthplace%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%20%20%23%20labels%20in%20English%0A%7D){:target="_blank"} demonstrieren kann.

Was all diese Konzepte am Ende der Woche zusammenbrachte, war das [F-TEMPO-Projekt](https://f-tempo.org/){:target="_blank"} von Tim Crawford, das es ermöglicht, aus all diesen digitalen Werkzeugen echte musikwissenschaftliche Forschung zu machen. Es handelt sich um eine Volltextsuche der Alten Musik mit Hilfe der ca. 320 digitalisierte Ausgaben (über 40.000 Seiten) von [Early Music Online](https://www.royalholloway.ac.uk/research-and-teaching/departments-and-schools/music/research/research-projects-and-centres/early-music-online/){:target="_blank"}. Es verwendet optische Musikerkennung (OMR), um ähnliche Muster zu finden. Hinter den Kulissen laufen [Aruspix](http://www.aruspix.net/){:target="_blank"} (für die OMR) und MEI. [In the Demo](http://www.doc.gold.ac.uk/usr/265/){:target="_blank"} können Sie eine Seite aus einer Anthologie nehmen und dann nach ähnlicher Musik suchen und so das gleiche Stück in anderen Anthologien finden - auch wenn ein anderer (oder sogar anonymer) Komponist benannt ist, oder in einer anderen Anordnung. Sie können versuchen, Ihre eigenen Bilder für die Suche hochzuladen!

Der faszinierende Faden, der durch die DHOxSS-Woche führte, war die Verwendung digitaler Tools, um neue und andere Fragen zu stellen. Dank der Menge an digitalisierten Daten, ob audio, notiert oder bibliographisch, gibt es Datenbestände ([einschließlich RISM](https://opac.rism.info/index.php?id=10){:target="_blank"}), die groß genug sind, um neue Untersuchungsgebiete zu erschließen.

​

