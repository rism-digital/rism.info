---
layout: post
title: Digital Musicology at Oxford Summer School
date: 2019-09-02
lang: en
post: true
category: events
image: "/images/news-old-website/csm_Oxford_Schola_Musicae_6ee2b1c702.jpg"
old_url: http://www.rism.info//home/newsdetails/browse/62/article/64/digital-musicology-at-oxford-summer-school.html
email: ''
author: Jennifer Ward (RISM Central Office)
---

I've been wanting to attend the [Digital Humanities at Oxford Summer School (DHOxSS)](https://www.dhoxss.net/){:target="_blank"} for some time now, thanks to glowing reviews from librarian friends. I was glad when my summer plans finally aligned with this year's Summer School, and directly after the [IAML Congress in Kraków](/events/2019/08/26/congress-diary-from-iaml-kraków-2019.html){:target="_blank"} I hopped over to Oxford. I was fortunate enough to receive a bursary to attend the [Digital Musicology strand](https://www.dhoxss.net/digital-musicology){:target="_blank"}, which was convened by Kevin Page. A report summarizing my week has been [published on the DHOxSS website](https://www.dhoxss.net/jennifer-ward){:target="_blank"}. In this space, I'd like to go into more detail about the tools and techniques we used.

The week was broadly divided into three modules that focused on digital audio, digital notation, and digital descriptions. The format of class sessions was mixed so that lectures by a variety of scholars alternated with chunks of time to practice new tools.

Digital audio processing involves analyzing sound files in an almost mathematical way, in that the end result is numbers that can be manipulated in a database. Along the way we dug deep into the field of music information retrieval (see [ISMIR](http://ismir.net/){:target="_blank"} and [MIREX](https://www.music-ir.org/mirex/wiki/MIREX_HOME){:target="_blank"}), n-grams, and the physics of audio waves. We used [Sonic Visualiser](https://www.sonicvisualiser.org/){:target="_blank"} and [Vamp plugins](https://vamp-plugins.org/){:target="_blank"} to extract features from sound recordings. Then, you can ask the computer to search for patterns, such as "In nomine" melodies in a corpus of Renaissance lute music. [Songle](https://songle.jp/){:target="_blank"} and [Chordify](https://chordify.net/){:target="_blank"} are other projects that can recognize audio patterns, the latter of which lets you play along with YouTube videos by displaying the chords. The [Baudelaire Song Project](https://www.baudelairesong.org/){:target="_blank"} has [visualization tools](https://visualisebaudelairesong.bham.ac.uk/){:target="_blank"} to let you browse 1,700 settings of the poet's texts by decade, theme, genre, etc.

Computers can also analyze notation, if they can be taught to read it first. Frauke Jürgensen uses [kern](https://www.humdrum.org/rep/kern/){:target="_blank"} and [Humdrum](https://www.humdrum.org/){:target="_blank"} to encode and process mensural notation, and then you can ask the computer to count vast amounts of intervals, cadences, and other aspects that would take a human an inordinate amount of time to undertake. We used the [Music Encoding Initiative (MEI)](https://music-encoding.org/){:target="_blank"} standards, the [Atom text editor](https://atom.io/){:target="_blank"}, the [Verovio viewer](https://www.verovio.org/){:target="_blank"}, and the [Music21 library](https://web.mit.edu/music21/){:target="_blank"} (using [Python/Jupyter](https://jupyter.org/){:target="_blank"}) to encode music details in a way that allows analysis to be performed.

The last theme of the workshop focused on linking knowledge: not merely throwing information on the web, but enriching discovery and analysis through connecting with other projects and datasets. This brings us into the world of the semantic web, the [linked open data cloud](https://lod-cloud.net/#){:target="_blank"} ([RISM is here](https://lod-cloud.net/dataset/rism){:target="_blank"}), the Resource Description Framework (RDF), SPARQL queries, and other acronyms you might have encountered if you are familiar with [Big Data concepts](https://doi.org/10.1093/em/cav071){:target="_blank"}. [Wikidata](https://query.wikidata.org){:target="_blank"} has an easily understandable interface that can demonstrate the power of linked data (click on Examples) with their [map of composers by birthplace](https://query.wikidata.org/#%23Music%20composers%20by%20birth%20place%0A%23defaultView%3AMap%0ASELECT%20%3Fitem%20%3FitemLabel%20%3F_coordinates%20%3F_image%20WHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP106%20wd%3AQ36834%3B%20%20%20%23%20occupation%3A%20composer%0A%20%20%20%20%20%20%20%20wdt%3AP18%20%3F_image%3B%20%20%20%23%20with%20an%20image%20depicting%20them%0A%20%20%20%20%20%20%20%20wdt%3AP19%2Fwdt%3AP625%20%3F_coordinates%20%20%20%23%20their%20birthplace%2C%20specifically%20the%20coordinates%20of%20their%20birthplace%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%20%20%23%20labels%20in%20English%0A%7D){:target="_blank"}.

What brought all these concepts together at the end of the week was the [F-TEMPO project](https://f-tempo.org/){:target="_blank"} by Tim Crawford, which enables true musicological research to come out of all these digital tools. It is a full-text search of early music, using the [ca. 320 digitized editions from Early Music Online](https://www.royalholloway.ac.uk/research-and-teaching/departments-and-schools/music/research/research-projects-and-centres/early-music-online/){:target="_blank"}, over 40,000 pages. It uses optical music recognition (OMR) to find similar patterns across the corpus. Running behind the scenes is [Aruspix](http://www.aruspix.net/){:target="_blank"} (for the OMR) and MEI. [In the demo](http://www.doc.gold.ac.uk/usr/265/){:target="_blank"}, you can take a page from an anthology and then search for similar music, thereby finding the same piece in other anthologies - even when a different (or even anonymous) composer is named, or in a different arrangement. You can try uploading your own images to search!

The fascinating thread that went through the DHOxSS week was using digital tools to ask new and different questions. Thanks to the amount of digitized data out there, whether audio, notational, or bibliographical, there are data pools ([including RISM's](https://opac.rism.info/index.php?id=10){:target="_blank"}) that are large enough to open up new areas of inquiry.

​


